# NLP-projects

### What Is Natural Language Processing?

- Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.

- The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.

- Given the importance of this type of data, we must have methods to understand and reason about natural language, just like we do for other types of data.

<img src="/assets/google-smith.jpeg" width="300" align="center"/>


### How Does Natural Language Processing Work?
- In natural language processing, human language is separated into fragments so that the grammatical structure of sentences and the meaning of words can be analyzed and understood in context. This helps computers read and understand spoken or written text in the same way as humans.

- Here are a few fundamental NLP pre-processing tasks data scientists need to perform before NLP tools can make sense of human language:

    * Tokenization: breaks down text into smaller semantic units or single clauses
    * Part-of-speech-tagging: marking up words as nouns, verbs, adjectives, adverbs, pronouns, etc
    * Stemming and lemmatization: standardizing words by reducing them to their root forms
    * Stop word removal: filtering out common words that add little or no unique information, for example, prepositions and articles (at, to, a, the).

Only then can NLP tools transform text into something a machine can understand. 

### Natural Language Processing Algorithms :

- Once your data has been pre-processed, it’s time to move onto the next step: building an NLP algorithm, and training it so it can interpret natural language and perform specific tasks.

- There are two main algorithms you can use to solve NLP problems:

    * A rule-based approach. Rule-based systems rely on hand-crafted grammatical rules that need to be created by experts in linguistics, or knowledge engineers. This was the earliest approach to crafting NLP algorithms, and it’s still used today.

    * Machine learning algorithms. Machine learning models, on the other hand, are based on statistical methods and learn to perform tasks after being fed examples (training data). 

- The biggest advantage of machine learning algorithms is their ability to learn on their own. You don’t need to define manual rules – instead machines learn from previous data to make predictions on their own, allowing for more flexibility.

- Machine learning algorithms are fed training data and expected outputs (tags) to train machines to make associations between a particular input and its corresponding output. Machines then use statistical analysis methods to build their own “knowledge bank” and discern which features best represent the texts, before making predictions for unseen data (new texts):
<img src="/assets/Capture.PNG" width="800" height="800" align="center"/>



